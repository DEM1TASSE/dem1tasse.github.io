---
title: "Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents"
date: 2024-03-01
selected: true
pub: "ICLR"
pub_date: "2025"
pub_last: ' <span class="badge badge-pill badge-primary">Oral (1.8%)</span>'
cover: /assets/images/covers/uground.png
abstract: >-
  We present SeeAct-V, a human-like, vision-only GUI agent framework, and UGround, a SOTA GUI visual grounding model trained on cost-effective synthetic data, marking the first practical demonstration of SOTA performance for vision-only GUI agents.
authors:
- Boyu Gou
- Ruohan Wang
- Boyuan Zheng
- Yanan Xie
- Cheng Chang
- Yiheng Shu
- Huan Sun
- Yu Su
links:
  Paper: https://arxiv.org/abs/2410.05243
  Code: https://github.com/OSU-NLP-Group/UGround
  Homepage: https://osu-nlp-group.github.io/UGround/
  Twitter: https://x.com/ysu_nlp/status/1844186560901808328
--- 